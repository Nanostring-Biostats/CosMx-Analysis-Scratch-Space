---
title: "Data preparation"
output: 
rmarkdown::html_vignette: 
toc: true
fig_width: 7 
fig_height: 8 
vignette: >
  %\VignetteIndexEntry{Loading flat files}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
<style>
p.caption {
  font-size: 1.5em;
}
</style>
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


First we'll load the packages we need. 
Importantly, we'll load "Matrix", which handles *sparse matrices*. 
When a matrix has mainly zeroes, which is generally the case for single cell data,
storing it in sparse matrix format has huge memory savings. Given the size of CosMx
data, it is vital to be memory-efficient. 

Note: we'll store our count matrices with cells in rows and genes in columns. 
Many protocols arrange their count matrix the other way around, 
but our approach here will be more performant for most operations. 

```{r libraries}
# necessary libraries
library(data.table) # for more memory efficient data frames
library(Matrix) # for sparse matrices like our counts matrix
```

Now let's see the files we have to work with:

```{r files}
# location of flat files:
flatfiledir <- "//nanostring.local/RND/analysis-files/pdanaher/vignette/flat files/"
dir(flatfiledir)
# where to write output:
outdir <- "//nanostring.local/RND/analysis-files/pdanaher/vignette/"
```
So there are two slides, each with 4 files: an expression matrix, cell metadata, cell polygons, and a map of fov positions

Now we'll load in the flat files and format as we want:

```{r loading}
### automatically get slide names:
filenames <- dir(flatfiledir)
metadatafilenames <- filenames[grepl("metadata", filenames)]
slidenames <- gsub("_metadata_file.csv.gz", "", metadatafilenames)
slidenames

#### load in metadata from each slide:
metadatalist <- sapply(slidenames, function(slidename) {
  tempdatatable <- data.table::fread(paste0(flatfiledir, "/", slidename, "_metadata_file.csv.gz"))
  tempdatatable$slidename <- slidename
  return(list(tempdatatable))
})

### harmonize metadata column names:
# get column names shared by all metadata files:
sharedcolumns <- colnames(metadatalist[[1]])
if (length(metadatalist) > 1) {
  for (i in 2:length(metadatalist)) {
    sharedcolumns <- intersect(sharedcolumns, colnames(metadatalist[[i]]))
  }
}
# reduce to only shared columns:
for (i in 1:length(metadatalist)) {
  metadatalist[[i]] <- metadatalist[[i]][, ..sharedcolumns]
}

### load in counts matrix from each slide:
countlist <- sapply(slidenames, function(slidename) {
  # load in counts as a data table:
  countsdatatable <- data.table::fread(paste0(flatfiledir, "/", slidename, "_exprMat_file.csv.gz"))
  return(list(countsdatatable))
 
})

### get shared genes:
sharedgenes <- colnames(countlist[[1]])
if (length(countlist) > 1) {
  for (i in 2:length(countlist)) {
    sharedgenes <- intersect(sharedgenes, colnames(countlist[[i]]))
  }
}
sharedgenes <- setdiff(sharedgenes, c("fov", "cell_ID"))

#### align counts matrices to match metadata cell IDs, and convert to sparse matrices:
for (i in 1:length(countlist)) {
  # align:
  metadata_cell_fov <- paste0(metadatalist[[i]]$fov, metadatalist[[i]]$cell_ID)
  counts_cell_fov <- paste0(countlist[[i]]$fov, countlist[[i]]$cell_ID)
  countlist[[i]] <- countlist[[i]][match(metadata_cell_fov, counts_cell_fov), ]
  # trim redundant columns:
  countlist[[i]] <- countlist[[i]][, ..sharedgenes]
  # convert to sparse matrix:
  triplets <- data.table::as.data.table(which(countlist[[i]] != 0, arr.ind = TRUE))
  triplets$value <- countlist[[i]][triplets$row, triplets$col]
  countlist[[i]] <- Matrix::sparseMatrix(i = triplets$row, j = triplets$col, x = triplets$value)
}

### condense metadata to a single data table:
metadata <- c()
counts <- c()

for (i in 1:length(countlist)) {
  counts <- rbind(counts, countlist[[i]])
  metadata <- rbind(metadata, metadatalist[[i]])
}
# add cell IDs to counts:
if(any(duplicated(metadata$cell_id))) {
  stop("Found duplicated cell IDs, probably from different slides. Make sure they're all unique.")
}
rownames(counts) <- metadata$cell_id
colnames(counts) <- sharedgenes

# add to metadata: replace slide-specific FOV ID with a unique FOV ID:
metadata$FOV <- paste0(metadata$slidename, "_", metadata$fov)
metadata$fov <- NULL

```

Our counts data includes all readouts, including controls. 
We'll split those out. They can be useful for deep QC explorations, but for most purposes we won't need the full negative control count matrices.

```{r removeconttols}
# isolate negative control matrices:
negcounts <- counts[, grepl("Negative", colnames(counts))]
falsecounts <- counts[, grepl("SystemControl", colnames(counts))]
# reduce counts matrix to only genes:
counts <- counts[, !grepl("Negative", colnames(counts)) & !grepl("SystemControl", colnames(counts))]
```


Now our data looks like this:

<INSERT CARTOON> 


Next we save the data as .RDS files. These files are smaller on disk (they 
benefit from more compression than flat files do), and they're faster to read
back into R.

```{r saving}
saveRDS(counts, file = paste0(outdir, "/processed_data/counts.RDS"))
saveRDS(negcounts, file = paste0(outdir, "/processed_data/negcounts.RDS"))
saveRDS(falsecounts, file = paste0(outdir, "/processed_data/falsecounts.RDS"))
saveRDS(metadata, file = paste0(outdir, "/processed_data/metadata.RDS"))
```

Now we have data in 3 locations:

- The RDS files we just created
- The flat files we began with
- The original data on AtoMx

This is unnecessary duplication. Once you've confirmed this script has run successfully,
you can safely delete the flat files - you won't be using them again. 
If you do need to go back to them, you can always export them from AtoMx again:
AtoMx acts as a persistent, authoritative source of unmodified data.

### Note on data size

A reasonably high-performance compute instance can generally handle a few slides of 
 CosMx data using the scripts we use here. But once studies get to millions of cells, they
 can overwhelm the memory available to most analysts. At this point, you have two options:

1. Use data formats that store data on disk, not in memory, e.g. TileDB, SeuratDisk, hdf5... 
These tools can be harder to use.
2. Analyze your data one tissue at a time. Put all tissues through a standard analysis pipeline, then, for multi-tissue analyses, judiciously load in just the data you need
for a given analysis. 

Neither option is terribly convenient, but having millions or tens of millions of cells 
to analyze is worth some hassle. 


