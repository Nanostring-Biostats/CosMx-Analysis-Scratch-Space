---
title: "Differential expression"
output: 
rmarkdown::html_vignette: 
toc: true
fig_width: 7 
fig_height: 8 
vignette: >
  %\VignetteIndexEntry{Differential expression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
<style>
p.caption {
  font-size: 1.5em;
}
</style>
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Introduction

Outline:

- gene screening
- fast model
- better: (describe basic recommendation of smiDE paper)
- show volcano plots
- comment on how you need a good effect size



```{r loading}
# packages:
library(pheatmap)

# load data:
mydir <- "//nanostring.local/RND/analysis-files/pdanaher/vignette/"
norm <- readRDS(paste0(mydir, "processed_data/norm.RDS"))
metadata <- readRDS(paste0(mydir, "/processed_data/metadata_with_spatialcontext.RDS")) 
celltype <- readRDS(paste0(mydir, "/processed_data/celltype.RDS"))
xy <- readRDS(paste0(mydir, "/processed_data/xy.RDS"))
```

## Filtering for analyzeable genes 

Because cell segmentation is imperfect, genes that have low expression in your cell type of interest -
but that have high expression in its neighboring cells - have unreliable expression profiles
and often emerge as strong false positives in DE analyses. See [here](https://github.com/Nanostring-Biostats/CosMx-Analysis-Scratch-Space/blob/Main/blog/segmentation%20errors%20and%20DE.md) for more. 

We are working on an R package to handle this concern optimally and conveniently.  
For now, the below code suffices to flag gene/cell type pairs at high risk of contamination from segmentation 
errors. We recommend omitting these gene/cell type pairs from DE analysis.

```{r filter}


analyzable <- matrix(TRUE, nrow = ncol(norm), ncol = length(unique(celltype)),
                     dimnames = list(colnames(norm), unique(celltype)))
```


## Defining a DE question

Differential expression models let us ask, "how does a cell type respond to its environment?"
A great many biological questions can be framed this way.

For our example, we'll consider the question, "how do mesangial cells change gene expression
in response to nearby macrophages?" 

Formalizing this, we might write the model: *E(y) = f(number of nearby macrophages)*.
This says that expected expression of a gene in our cell type (*y*) is some function of the
number of nearby macrophages. 

We have a simple enough question, and a correspondingly simple model. 
But we should remember that mesangial cells are probably responding to many factors in their environment, not just
macrophages. If we omit any of these factors from our DE model, it will be incomplete,
with the omitted variable acting as a unmodeled confounder, biasing our results.
Biology is complex, and we'll never manage to specify a complete model for the local 
factors acting on podocyte gene expression, but we should try our best to capture
the major factors. 

So instead, we might try the following model:

*E(y) = f(number of nearby macrophages, local expression of inflammatory cytokines, number of nearby mesangial cells, spatial cluster)*.

This model accounts for other immune impacts on podocyte expression and for any impact
of glomerulus composition on mesangial cells. (Mesangial cells live alongside podocytes and glomerular endothelial cells in glomeruli.)
It also adjust for spatial cluster, which is reasonable to do whenever your variable of interest
is largely uncorrelated with spatial cluster. In this example, if one of the spatial clusters was
driven by local macrophage density, it would confuse interpretation of our results. 

Finally, we have the disadvantage of not many tissues in our study, but 
the benefit of many cells per tissue. In this setting, it becomes useful
to run our model separately for each tissue. Then we can see whether any 
genes are consistently DE across tissues. This cross-tissue comparison 
gives us confidence that our findings aren't idiosyncratic to any single tissue.
It also reveals interesting between-tissue heterogeneity. 


## Very fast exploratory DE

The most accurate DE models can take a long time to run. 
If you want to quickly explore a DE question, with imperfect accuracy, you
can use the below code, which uses matrix algebra to simultaneously run a linear 
model for every gene in your data.

To be clear: if you really want to hang your hat on a p-value, we recommend
a more principled model than the simple linear models implemented below. 


``` {r fastDE}
# subset on only mesangial cells:
inds <- celltype == "Mesangial.cell"

# subset on only the analyzable genes \
# (those expressed above contamination as calculated above)
usegenes <- names(which(analyzable[, "Mesangial.cell"]))
length(usegenes)

# get the design matrix to be used in every gene's model:
templm = lm(norm[inds, 1] ~ metadata$n_mac_neighbors[inds] + metadata$n_glom_endo_neighbors[inds] + metadata$n_lymphoid_neighbors[inds] + as.factor(metadata$spatialclust[inds]))
X = model.matrix(templm)
head(X)
# clean up column names in the design matrix:
colnames(X) <- gsub("metadata\\$", "", colnames(X))
colnames(X) <- gsub("\\[inds\\]", "", colnames(X))
colnames(X) <- gsub("\\)", "", colnames(X))
colnames(X) <- gsub("\\(", "", colnames(X))
colnames(X) <- gsub("as.factor", "", colnames(X))
colnames(X)

# now use matrix algebra to run a lm for every gene at once:
# get the estimates:
beta <- as.matrix(solve(t(X) %*% X) %*% t(X) %*% norm[inds, usegenes])
beta[, 1:5]
# get the SEs:
resids <- norm[inds, usegenes] - X %*% beta
rss <- colSums(resids^2)
sigmasquared <- rss / (nrow(X) - ncol(X))
ses <- sqrt(t(t(diag(solve(t(X) %*% X)))) %*% sigmasquared)
# and the p-value:
ps <- 2 * (1 - pt(abs(beta) / ses, df = nrow(X) - ncol(X)))

if (FALSE) {   # THIS NEEDS REVISION
  # convert from linear change in expression to a log2(fold-change):
  binaryvariables = paste0("spatialclust", 2:6)
  continuousvariables = c("n_mac_neighbors", "n_glom_endo_neighbors", "n_lymphoid_neighbors")
  log2fc = beta * NA
  # for binary variables, record the fold-change associated with the variable:
  for (name in binaryvariables) {
    log2fc[name, ] <- log2((beta[name, ] + beta["Intercept", ]) / beta["Intercept", ])
  }
  # for continuous variables, record the fold-change associated with a 1 SD increase in the variable:
  for (name in binaryvariables) {
    log2fc[name, ] <- log2((beta[name, ] * sd(X[, name]) +  beta["Intercept", ]) / beta["Intercept", ])
  }
}

## confirm it reproduces lm():
# basic lm results:
summary(templm)$coef[, c("Estimate", "Std. Error", "Pr(>|t|)")]
# our matrix algebra results:
cbind(beta[, 1], ses[, 1], ps[, 1])


## a nice way to look at results across several variables and all genes:
## heatmap of only the significant effect sizes:
pheatmap(beta * (ps < 1e-2), col = colorRampPalette(c("darkblue", "blue", "white", "red", "darkred"))(100),
         breaks = seq(-0.5, 0.5, length.out = 101))



```


## More optimal DE

Our upcoming DE R package and manuscript will provide an in-depth discussion of DE modeling. 
For now, the below code performs slower but more optimal DE modeling. 
For publishable results, we recommend this above the simple linear models implemented above. 

```{r optimalde}


```


## Plotting and exploring DE results

MORE NEEDED HERE

```{r plotting}

## a nice way to look at results across several variables and all genes:
## heatmap of only the significant effect sizes:
pheatmap(beta * (ps < 1e-2), col = colorRampPalette(c("darkblue", "blue", "white", "red", "darkred"))(100),
         breaks = seq(-0.5, 0.5, length.out = 101))

```

## Notes on interpretation

Thanks to the huge number of cells in most experiments, DE analysis in CosMx data
often returns spectacularly significant p-values. Sadly, these should not be taken
as irrefutably convincing evidence. High sample size (number of cells) gives high power 
to detect changes in biology, but also high power to detect biases induced by technical 
artifacts and by biological variables omitted from DE models. 
In other words, if your model omits any confounder (as all models inevitably do),
a DE analysis on tens of thousands of cells will easily find genes impacted by 
this confounder to be "significant".

The countermeasure to this unfortunate statistical power is to evaluate genes based on 
their effect size, not on their p-values alone. While a minor confounder leads to 
smaller and smaller p-values as N increases, the bias it induces in a gene's effect
size will remain small. 

In short, act as if you model accounts for major influences on gene expression but
omits many minor influances, and only get excited about large effect sizes. 
A good rule of thumb is that a change in expression of +/-20% is believable; 
for effect sizes smaller than this some skepticism is advised.