---
title: "QC, normalization and batch-correction"
output: 
rmarkdown::html_vignette: 
toc: true
fig_width: 7 
fig_height: 8 
vignette: >
  %\VignetteIndexEntry{QC, normalization and batch-correction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
<style>
p.caption {
  font-size: 1.5em;
}
</style>
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## QC

QC of CosMx data is mostly straightforward. Technical effects can be complex,
 but they manifest in limited ways:
 
- Many individual cells can have low signal
- Segmentation errors can create "cells" with bad data
- Sporadic FOVs can have low signal
- Sporadic FOVs can have distorted / outlier expression profiles


First we'll flag cells with poor signal, then we'll flag FOVs. 
When we're done, we'll delete everything we've flagged. 
(The full original data will remain on AtoMx, so this step is not as drastic as it seems. 
For smaller studies it could still be reasonable to hold on to the pre-QC .RDS files.)

The QCs implemented here mimic and expand on those of AtoMx. 
We reimplement AtoMx QCs here to give you more granular control on how they're implemented. 

We start by loading the data we'll need:

```{r loaddata}
mydir <- "//nanostring.local/RND/analysis-files/pdanaher/vignette/"
metadata <- readRDS(paste0(mydir, "/processed_data/metadata_unfiltered.RDS")) 
counts <- readRDS(paste0(mydir, "/processed_data/counts_unfiltered.RDS")) 
negcounts <- readRDS(paste0(mydir, "/processed_data/negcounts_unfiltered.RDS")) 
falsecounts <- readRDS(paste0(mydir, "/processed_data/falsecounts_unfiltered.RDS")) 
xy <- readRDS(paste0(mydir, "/processed_data/xy_unfiltered.RDS"))
```

### Cell-level QC

We'll check for two kinds of bad cells: those with too few transcripts to use, 
and those that look to result from bad segmentation errors.

We generally require 20 counts per cell in 1000plex data and 50 counts in 6000plex data:

```{r countqc}
# require 50 counts per cell 
count_threshold <- 50
metadata$totalcounts <- Matrix::rowSums(counts)
flag <- metadata$totalcounts < count_threshold
```

Then we'll look for very large cells. You can run a Grubb's test to look for high outliers
of cell area, or you can determine a reasonable threshold yourself:

```{r areaqc}
# what's the distribution of areas?
hist(metadata$Area, breaks = 100, xlab = "Cell Area", main = "")
# based on the above, set a threshold:
area_threshold <- 21000
abline(v = area_threshold, col = "red")
# flag cells based on area:
flag <- flag | (metadata$Area > area_threshold)
```

### FOV-level QC:

A spatial map of per-cell signal total counts usually makes poor FOVs quite obvious. 
Even better, we can color cells by the average counts of their spatial neighbors 
within their FOV:

```{r smoothedtotalcounts, fig.dim = c(7, 7)}
source("utils/spatial functions.R")
# get cells' spatial neighbors (within their FOV):
neighbors <- nearestNeighborGraph(x = xy[, 1], y = xy[, 2], N = 20, subset = metadata$FOV)
# now get mean counts of neighbors:
neighborcounts <- neighbor_mean(x = metadata$totalcounts, neighbors = neighbors)
lowerlimit <- max(log2(1000), min(log2(neighborcounts)))
# now plot it:
plot(xy, pch = 16, cex = 0.1, 
     asp = 1, # important: keep the true aspect ratio
     col = viridis::viridis_pal(option = "B")(101)[
       pmax(pmin(1 + round(100 * (log2(neighborcounts) - lowerlimit) / (max(log2(neighborcounts)) - lowerlimit)), 101), 1)])
# label FOVs:
#for (fov in unique(metadata$FOV)) {
#  text(median(xy[metadata$FOV == fov, 1]), median(xy[metadata$FOV == fov, 2]), fov)
#}
```


We don't see any low-signal FOVs in this plot, but if we did, we could flag the cells in them as follows.
Note that first you'd want to confirm that their signal was truly poor and not just slightly attenuated vs. their neighbors. It's OK for signal to vary; it just seems prudent to remove true outliers.

```{r lowcountfovs}
low_count_fovs <- c("1121", "1259") 
flag <- flag | is.element(metadata$FOV, low_count_fovs)
```


Next, we'll look for FOVs with distorted expression profiles. 
Below we color cells by their neighborhood expression values, allowing for distorted FOVs to stand out.

```{r distortedfovs, fig.dim = c(7, 7)}
tempumapfileloc <- paste0(mydir, "/processed_data/neighborumap.RDS")
if (!file.exists(tempumapfileloc)) {
  # operate on a subsample to save time:
  set.seed(0)
  nsub <- length(unique(metadata$FOV)) * 100
  sub <- sample(1:nrow(metadata), nsub)

  # get the mean expression profile around each cell:
  neighbormeanexpression <- neighbor_colMeans(x = counts, neighbors = neighbors[sub, ]) 
  # normalize it:
  neighbormeanexpression <- sweep(neighbormeanexpression, 1, Matrix::rowSums(neighbormeanexpression), "/")

  # project to 3 umap dimensions:
  temppc <- prcomp(neighbormeanexpression)$x[, 1:20]
  tempum <- uwot::umap(temppc, n_components = 3) # Note: a common error in uwot::umap is resolved with: install.packages("Matrix", type = "source"); install.packages("irlba",   type = "source")
  # rescale on 0.05 - 0.95:
  tempum <- apply(tempum, 2, function(x) {
    pmax(pmin((x - quantile(x, 0.05)) / (quantile(x, 0.95) - quantile(x, 0.05)), .95), 0.05)
  })
  saveRDS(tempum, file = tempumapfileloc)
} else {
  tempum <- readRDS(tempumapfileloc)
}


# color by 3 umap dimensions:
plot(xy[sub, ], pch = 16, cex = 0.2, 
     asp = 1, # important: keep the true aspect ratio
     col = rgb(tempum[, 1], tempum[, 2], tempum[, 3], 1),
     main = "Cellular neighborhoods colored by their UMAP projection")
# label FOVs:
#for (fov in unique(metadata$FOV)) {
#  text(median(xy[metadata$FOV == fov, 1]), median(xy[metadata$FOV == fov, 2]), fov)
#}
```


Here we see two major drivers of spatial signal. First, small round glomeruli (these are kidneys)
 emerge as distinct from their surroundings. And second, different tissues are projected to different
 regions of the UMAP. 
We're looking for a third effect: FOVs that are discordant from their peers. 
These would suggest FOV-level artifacts, and we'd usually want to flag and remove those FOVs. 

We do see what looks like an edge effect atop many FOVs (this was more common in very early datasets like this one.)
It would be reasonable to remove cells in the top edge of these FOVs.
 
Example for for flagging FOVs we pick out as discordant from the above plot:

```{r flagdistortedfovs}
distorted_fovs <- c("1549", "1432") 
flag <- flag | is.element(metadata$FOV, distorted_fovs)
```

### Remove flagged cells and FOVs

Now that we've flagged cells we don't want to analyze, we remove them from all of our data objects. 
To avoid risk of data misalignment, we'll use cell IDs to coordinate this operation. 

```{r remove}
# how many cells are we flagging?
table(flag)
# keep these cells:
keepcells <- metadata$cell_id[!flag]
# subset all data objects to only the cells to be kept:
counts <- counts[keepcells, ]
negcounts <- negcounts[keepcells, ]
falsecounts <- falsecounts[keepcells, ]
metadata <- metadata[match(keepcells, metadata$cell_id), ]
xy <- xy[keepcells, ]
# overwrite saved data with filtered data:
saveRDS(counts, paste0(mydir, "/processed_data/counts.RDS"))
saveRDS(negcounts, paste0(mydir, "/processed_data/negcounts.RDS"))
saveRDS(falsecounts, paste0(mydir, "/processed_data/falsecounts.RDS"))
saveRDS(metadata, paste0(mydir, "/processed_data/metadata.RDS"))
saveRDS(xy, paste0(mydir, "/processed_data/xy.RDS"))
```

## Normalization

Normalization is straightforward: we simply divide every cell's expression profile by its total counts.
To put our counts back on a more natural scale, we then multiply all cells by the mean
cell's total counts:

```{r normalize}
norm <- sweep(counts, 1, metadata$totalcounts, "/") * mean(metadata$totalcounts)
saveRDS(norm, paste0(mydir, "/processed_data/norm.RDS"))
```

