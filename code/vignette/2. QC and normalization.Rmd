---
title: "QC normalization and batch-correction"
output: 
rmarkdown::html_vignette: 
fig_width: 7 
fig_height: 8 
vignette: >
  %\VignetteIndexEntry{QC normalization and batch-correction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
<style>
p.caption {
  font-size: 1.5em;
}
</style>
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## QC

QC of CosMx data is mostly straightforward. Technical effects can be complex,
 but they manifest in limited ways:
 
- Many individual cells can have low signal
- Segmentation errors can create "cells" with bad data
- Sporadic FOVs can have low signal
- Sporadic FOVs can have distorted / outlier expression profiles


First we'll flag cells with poor signal, then we'll flag FOVs. 
When we're done, we'll delete everything we've flagged. 
(The full original data will remain on AtoMx, so this step is not as drastic as it seems. 
For smaller studies it could still be reasonable to hold on to the pre-QC .RDS files.)

The QCs implemented here mimic and expand on those of AtoMx. 
We reimplement AtoMx QCs here to give you more granular control on how they're implemented. 

We start by loading the data we'll need:

```{r loaddata}
# note these files are for convenience during analysis, and are not a NanoString-supported format
mydir <- "../"
metadata <- readRDS(paste0(mydir, "/processed_data/metadata_unfiltered.RDS")) 
counts <- readRDS(paste0(mydir, "/processed_data/counts_unfiltered.RDS")) 
negcounts <- readRDS(paste0(mydir, "/processed_data/negcounts_unfiltered.RDS")) 
falsecounts <- readRDS(paste0(mydir, "/processed_data/falsecounts_unfiltered.RDS")) 
xy <- readRDS(paste0(mydir, "/processed_data/xy_unfiltered.RDS"))
```

### Cell-level QC

We'll check for two kinds of bad cells: those with too few transcripts to use, 
and those that look to result from bad segmentation errors.

We generally require 20 counts per cell in 1000plex data and 50 counts in 6000plex data:

```{r countqc}
# require 50 counts per cell 
count_threshold <- 20
metadata$totalcounts <- Matrix::rowSums(counts)
flag <- metadata$totalcounts < count_threshold
table(flag)
```

Then we'll look for very large cells. You can run a Grubb's test to look for high outliers
of cell area, or you can determine a reasonable threshold yourself:

```{r areaqc}
# what's the distribution of areas?
hist(metadata$Area, breaks = 100, xlab = "Cell Area", main = "")
# based on the above, set a threshold:
area_threshold <- 30000
abline(v = area_threshold, col = "red")
# flag cells based on area:
flag <- flag | (metadata$Area > area_threshold)
table(flag)
```


### FOV-level QC: low-signal FOVs: 

A spatial map of per-cell signal total counts usually makes poor FOVs quite obvious. 
Even better, we can color cells by the average counts of their spatial neighbors 
within their FOV:

```{r smoothedtotalcounts, fig.dim = c(7, 7)}
source("utils/spatial functions.R")
# get cells' spatial neighbors (within their FOV):
neighbors <- 1 * (nearestNeighborGraph(x = xy[, 1], y = xy[, 2], N = 20, subset = metadata$FOV) != 0)
# now get mean counts of neighbors:
neighborcounts <- neighbor_mean(x = metadata$totalcounts, neighbors = neighbors)
lowerlimit <- max(log2(50), min(log2(neighborcounts)))
# now plot it:
plot(xy, pch = 16, cex = 0.1, 
     asp = 1, # important: keep the true aspect ratio
     col = viridis::viridis_pal(option = "B")(101)[
       pmax(pmin(1 + round(100 * (log2(neighborcounts) - lowerlimit) / (max(log2(neighborcounts)) - lowerlimit)), 101), 1)])
legend("topright", pch = 16, col = c(NA, viridis::viridis_pal(option = "B")(101)[c(1,50,101)]),
       legend = c("Mean neighborhood counts per cell:", round(c(2^lowerlimit, 2^mean(c(lowerlimit, max(log2(neighborcounts)))), max(neighborcounts)))))
# label FOVs:
#for (fov in unique(metadata$FOV)) {
#  text(median(xy[metadata$FOV == fov, 1]), median(xy[metadata$FOV == fov, 2]), fov)
#}
```

We do see regions of low signal, but they appear to vary smoothly over space, tracing tissue biology, not FOV borders.
If we did see FOVs where signal dropped, we could flag the cells in them as follows.
Note that first you'd want to confirm that their signal was truly poor and not just slightly attenuated vs. their neighbors. It's OK for signal to vary; only true outliers need to be removed.

```{r lowcountfovs}
low_count_fovs <- c("exampleFOV101", "exampleFOV342") 
flag <- flag | is.element(metadata$FOV, low_count_fovs)
table(flag)
```


### FOV-level QC: check for distorted gene expression profiles:

Above we checked for FOVs with an overall loss in signal. 
However, there are other, more subtle, FOV error modes that cause loss of counts in just subsets of genes. 
Here we search for FOVs showing this behavior. 

A new (as of April 2024) approach to FOV QC is available here: https://github.com/Nanostring-Biostats/CosMx-Analysis-Scratch-Space/code/FOV%20QC

A description of the method is found here: https://github.com/Nanostring-Biostats/CosMx-Analysis-Scratch-Space/blob/FOV-QC/blog/FOV%20QC.md

To summarize briefly: for each "bit" in our barcodes (e.g. red in reporter cycle 8), we look for FOVs where 
genes using the bit are underexpressed. 

```{r fovqc}
# get the FOV QC utils code:
source("https://raw.githubusercontent.com/Nanostring-Biostats/CosMx-Analysis-Scratch-Space/FOV-QC/code/FOV%20QC/FOV%20QC%20utils.R")

# get barcode details for this panel:
allbarcodes <- readRDS(url("https://github.com/Nanostring-Biostats/CosMx-Analysis-Scratch-Space/raw/FOV-QC/code/FOV%20QC/barcodes_by_panel.RDS"))
names(allbarcodes)
# get the barcodes for the panel we want:
barcodemap <- allbarcodes$Hs_UCC
head(barcodemap)

# run the method:
fovqc <- runFOVQC(counts = counts, xy = xy, fov = metadata$FOV, barcodemap = barcodemap, max_prop_loss = 0.33)
# report flagged FOVs:
fovqc$flaggedfovs

# see how expression of genes with impacted bits changes:
FOVEffectsSpatialPlots(fovqc, outdir = NULL, bits = "flagged") 

# remove cells in those FOVs:
flag <- flag | is.element(metadata$FOV, fovqc$flaggedfovs)
table(flag)
```




### Remove flagged cells and FOVs

Now that we've flagged cells we don't want to analyze, we remove them from all of our data objects. 
To avoid risk of data misalignment, we'll use cell IDs to coordinate this operation. 

```{r remove}
# how many cells are we flagging?
table(flag)
# subset all data objects to only the cells to be kept:
counts <- counts[!flag, ]
negcounts <- negcounts[!flag, ]
falsecounts <- falsecounts[!flag, ]
metadata <- metadata[!flag, ]
xy <- xy[!flag, ]
# overwrite saved data with filtered data:
# note these files are for convenience during analysis, and are not a NanoString-supported format
saveRDS(counts, paste0(mydir, "/processed_data/counts.RDS"))
saveRDS(negcounts, paste0(mydir, "/processed_data/negcounts.RDS"))
saveRDS(falsecounts, paste0(mydir, "/processed_data/falsecounts.RDS"))
saveRDS(metadata, paste0(mydir, "/processed_data/metadata.RDS"))
saveRDS(xy, paste0(mydir, "/processed_data/xy.RDS"))
```

## Normalization

Normalization is straightforward: we simply divide every cell's expression profile by its total counts.
To put our counts back on a more natural scale, we then multiply all cells by the mean
cell's total counts. 


```{r normalize}
norm <- sweep(counts, 1, metadata$totalcounts, "/") * mean(metadata$totalcounts)
saveRDS(norm, paste0(mydir, "/processed_data/norm.RDS"))
```

Note: if you'll analyze many tissues using the same pipeline, then you should 
replace the `mean(metadata$totalcounts)` term with some fixed value that is the 
same for all samples. Mathematically all positive values are the same, but entering
a value approximately equal to the average cell's total counts will put the data
on an easily-interpretable scale. 
