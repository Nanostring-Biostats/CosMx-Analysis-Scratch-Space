---
title: "QC and normalization"
output: 
rmarkdown::html_vignette: 
toc: true
fig_width: 7 
fig_height: 8 
vignette: >
  %\VignetteIndexEntry{Straightforward InSituCor run}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
<style>
p.caption {
  font-size: 1.5em;
}
</style>
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## QC

QC of CosMx data is mostly straightforward. Technical effects can be complex,
 but they manifest in limited ways:
 
- Many individual cells can have low signal
- Segmentation errors can create "cells" with bad data
- Sporadic FOVs can have low signal
- Spordaic FOVs can have distorted / outlier expression profiles


First we'll flag cells with poor signal, then we'll flag FOVs. 
When we're done, we'll delete everything we've flagged. 
(The full original data will remain on AtoMx, so this step is not as drastic as it seems. 
For smaller studies it could still be reasonable to hold on to the pre-QC .RDS files.)

We start by loading the data we'll need:

```{r loaddata}
counts <- readRDS()
annot <- readRDS()
xy <- readRDS()
xy0 <- readRDS()
```

### Cell-level QC

We'll check for two kinds of bad cells: those with too few transcripts to use, 
and those that look to result from bad segmentation errors.

We generally require 20 counts per cell in 1000plex data and 50 counts in 6000plex data:

```{r countqc}
# require 50 counts per cell 
count_threshold <- 50
annot$totalcounts <- Matrix::rowSums(counts)
flag <- annot$totalcounts < count_threshold
```

Then we'll look for very large cells. You can run a Grubb's test to look for high outliers
of cell area, or you can determine a reasonable threshold yourself:

```{r areaqc}
# what's the distribution of areas?
hist(annot$area, breaks = 100, xlim = c(0, 0.5))
# based on the above, set a threshold:
area_threshold <- ___
abline(v = area_threshold, col = "red")
# flag cells based on area:
flag <- flag | (annot$area > area_threshold)
```

### FOV-level QC:

A spatial map of per-cell signal total counts usually makes poor FOVs quite obvious. 
Even better, we can color cells by the average counts of their spatial neighbors 
within their FOV:

```{r smoothedtotalcounts}
source("utils/spatial functions.R")
# get cells' spatial neighbors (within their FOV):
neighbors <- nearestNeighborGraph(x = xy[, 1], y = xy[, 2], N = 20, subset = annot$FOV)
# now get mean counts of neighbors:
neighborcounts <- neighbor_mean(x = annot$totalcounts, neighbors = neighbors)
# now plot it:
plot(xy, pch = 16, cex = 0.2, 
     asp = 1, # important: keep the true aspect ratio
     col = viridis::viridis_pal(option = "B")(101)[
       1 + round(100 * log2(neighborcounts) / max(log2(neighborcounts)))])
# label FOVs:
for (fov in unique(annot$FOV)) {
  text(median(xy[annot$FOV == fov, 1]), median(xy[annot$FOV == fov, 2]), fov)
}
```


We don't see any low-signal FOVs in this plot, but if we did, we could flag the cells in them as follows.
Note that first you'd want to confirm that their signal was truly poor and not just slightly attenuated vs. their neighbors. It's OK for signal to vary; it just seems prudent to remove true outliers.

```{r lowcountfovs}
low_count_fovs <- c("1121", "1259") 
flag <- flag | is.element(annot$FOV, low_count_fovs)
```


Next, we'll look for FOVs with distorted expression profiles. 
Below we color cells by their neighborhood expression values, allowing for distorted FOVs to stand out.

```{r distortedfovs}
# operate on a subsample to save time:
nsub <- length(unique(annot$fov)) * 200
sub <- sample(1:nrow(annot), nsub)

# get the mean expression profile around each cell:
neighbormeanexpression <- neighbor_colMeans(x = counts[sub, ], neighbors = neighbors) 

# project to 3 umap dimensions:
temppc <- prcomp(neighbormeanexpression)$x[, 1:20]
tempum <- uwot::umap(temppc, n_components = 3)
# rescale on 0.05 - 0.95:
tempum <- apply(tempum, 2, function(x) {
  pmax(pmin((x - quantile(x, 0.05) / (quantile(x, 0.95) - quantile(x, 0.05))), .95), 0.05)
})

# color by 3 umap dimensions:
plot(xy, pch = 16, cex = 0.2, 
     asp = 1, # important: keep the true aspect ratio
     col = rgb(tempum[, 1], tempum[, 2], tempum[, 3], 1))
# label FOVs:
for (fov in unique(annot$FOV)) {
  text(median(xy[annot$FOV == fov, 1]), median(xy[annot$FOV == fov, 2]), fov)
}
```


Here we see that ________. 
Now flag those FOVs:

```{r flagdistortedfovs}
distorted_fovs <- c("1549", "1432") 
flag <- flag | is.element(annot$FOV, distorted_fovs)
```

### Remove flagged cells and FOVs

Now that we've flagged cells we don't want to analyze, we remove them from all of our data objects. 
To avoid risk of data misalignment, we'll use cell IDs to coordinate this operation. 

```{r remove}
# how many cells are we flagging?
table(flag)
# keep these cells:
keepcells <- annot$cell_ID[!flag]
# subset all data objects to only the cells to be kept:
counts <- counts[keepcells, ]
annot <- annot[keepcells, ]
xy <- xy[keepcells, ]
xy0 <- xy0[keepcells, ]
# overwrite saved data with filtered data:
saveRDS(counts, "______")
saveRDS(annot, "______")
saveRDS(xy, "______")
saveRDS(xy0, "______")
```

## Normalization

Normalization is straightforward: we just scale all cells to have the same total counts.

```{r normalize}
norm <- sweep(counts, 1, annot$totalcounts, "/")
saveRDS(norm, "processed_data/norm.RDS")
```


