---
title: "Data preparation"
output: 
rmarkdown::html_vignette: 
fig_width: 7 
fig_height: 8 
vignette: >
  %\VignetteIndexEntry{Loading flat files}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
<style>
p.caption {
  font-size: 1.5em;
}
</style>
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


First we'll load the packages we need. 
Importantly, we'll load "Matrix", which handles *sparse matrices*. 
When a matrix has mainly zeroes, which is generally the case for single cell data,
storing it in sparse matrix format has huge memory savings. Given the size of CosMx
data, it is vital to be memory-efficient. 

Note: we'll store our count matrices with cells in rows and genes in columns. 
Many protocols arrange their count matrix the other way around, 
but our approach here will be more performant for most operations. 

```{r libraries}
# necessary libraries
library(data.table) # for more memory efficient data frames
library(Matrix) # for sparse matrices like our counts matrix
```

Now let's see the files we have to work with:

```{r files}
# location of flat files:
myflatfiledir <- "../data/flatFiles"
dir(myflatfiledir)
# where to write output:
outdir <- ".."
```
So there are two slides, each with 4 files: an expression matrix, cell metadata, cell polygons, and a map of fov positions.
Note we have omitted the transcript locations file from this analysis; that data can be useful for visualizations,
but is unnecessary for the analyses we demonstrate here.

Now we'll load in the flat files and format as we want.

Trying to load an entire matrix into memory at once, may cause out-of-memory issues for large files.  
Here's some code below which gives an example of loading in the counts matrices in chunks.  
* First, we'll use `data.table::fread` function which can directly read gzipped files, and determine how many rows and columns we're working with.  
* Then, we'll read each chunk of data in, and convert it to a low-memory sparse matrix format before moving onto the next chunk.  
* At the end, we'll combine our sparse matrices together.

```{r}
### automatically get slide names:
slidenames <- dir(myflatfiledir)

### lists to collect the counts matrices and metadata, one per slide
countlist <- vector(mode='list', length=length(slidenames)) 
metadatalist <- vector(mode='list', length=length(slidenames)) 

for(i in 1:length(slidenames)){
   
  slidename <- slidenames[i] 

  msg <- paste0("Loading slide ", slidename, ", ", i, "/", length(slidenames), ".")
  message(msg)    
  # slide-specific files:
  thisslidesfiles <- dir(paste0(myflatfiledir, "/", slidename))
  
  # load in metadata:
  thisslidesmetadata <- thisslidesfiles[grepl("metadata\\_file", thisslidesfiles)]
  tempdatatable <- data.table::fread(paste0(myflatfiledir, "/", slidename, "/", thisslidesmetadata))
  
  # numeric slide ID 
  slide_ID_numeric <- tempdatatable[1,]$slide_ID 
    
  # load in counts as a data table:
  thisslidescounts <- thisslidesfiles[grepl("exprMat\\_file", thisslidesfiles)]
  
  countsfile <- paste0(myflatfiledir, "/", slidename, "/", thisslidescounts)
  nonzero_elements_perchunk <- 5*10**7
  ### Safely read in the dense (0-filled ) counts matrices in chunks.
  ### Note: the file is gzip compressed, so we don't know a priori the number of chunks needed.
  lastchunk <- FALSE 
  skiprows <- 0
  chunkid <- 1
  
  required_cols <- fread(countsfile, select=c("fov", "cell_ID"))
  stopifnot("columns 'fov' and 'cell_ID' are required, but not found in the counts file" = 
              all(c("cell_ID", "fov") %in% colnames(required_cols)))
  number_of_cells <- nrow(required_cols)
  
  number_of_cols <-  ncol(fread(countsfile, nrows = 2))
  number_of_chunks <- ceiling(number_of_cols * number_of_cells / (nonzero_elements_perchunk))
  chunk_size <- floor(number_of_cells / number_of_chunks)
  sub_counts_matrix <- vector(mode='list', length=number_of_chunks)
   
  pb <- txtProgressBar(min = 0, max = number_of_chunks, initial = 0, char = "=",
                       width = NA, title, label, style = 3, file = "")
  cellcount <- 0
  while(lastchunk==FALSE){
    read_header <- FALSE
    if(chunkid==1){
      read_header <- TRUE
    }
    
    countsdatatable <- data.table::fread(countsfile,
                                         nrows=chunk_size,
                                         skip=skiprows + (chunkid > 1),
                                         header=read_header)
    if(chunkid == 1){
      header <- colnames(countsdatatable)
    } else {
      colnames(countsdatatable) <- header
    }
     
    cellcount <- nrow(countsdatatable) + cellcount     
    if(cellcount == number_of_cells) lastchunk <- TRUE
    skiprows <- skiprows + chunk_size
    slide_fov_cell_counts <- paste0("c_",slide_ID_numeric, "_", countsdatatable$fov, "_", countsdatatable$cell_ID)
    sub_counts_matrix[[chunkid]] <- as(countsdatatable[,-c("fov", "cell_ID"),with=FALSE], "sparseMatrix") 
    rownames(sub_counts_matrix[[chunkid]]) <- slide_fov_cell_counts 
    setTxtProgressBar(pb, chunkid)
    chunkid <- chunkid + 1
  }
  
  close(pb)   
  
  countlist[[i]] <- do.call(rbind, sub_counts_matrix) 
  
  # ensure that cell-order in counts matches cell-order in metadata   
  slide_fov_cell_metadata <- paste0("c_",slide_ID_numeric, "_", tempdatatable$fov, "_", tempdatatable$cell_ID)
  countlist[[i]] <- countlist[[i]][match(slide_fov_cell_metadata, rownames(countlist[[i]])),] 
  metadatalist[[i]] <- tempdatatable 
  
  # track common genes and common metadata columns across slides
  if(i==1){
    sharedgenes <- colnames(countlist[[i]]) 
    sharedcolumns <- colnames(tempdatatable)
  }  else {
    sharedgenes <- intersect(sharedgenes, colnames(countlist[[i]]))
    sharedcolumns <- intersect(sharedcolumns, colnames(tempdatatable))
  }
    
}

# reduce to shared metadata columns and shared genes
for(i in 1:length(slidenames)){
  metadatalist[[i]] <- metadatalist[[i]][, ..sharedcolumns]
  countlist[[i]] <- countlist[[i]][, sharedgenes]
}
 
counts <- do.call(rbind, countlist)
metadata <- rbindlist(metadatalist)

# add to metadata: add a global non-slide-specific FOV ID:
metadata$FOV <- paste0("s", metadata$slide_ID, "f", metadata$fov)

# remove cell_ID metadata column, which only identifies cell within slides, not across slides:
metadata$cell_ID <- NULL

# isolate negative control matrices:
negcounts <- counts[, grepl("Negative", colnames(counts))]
falsecounts <- counts[, grepl("SystemControl", colnames(counts))]

# reduce counts matrix to only genes:
counts <- counts[, !grepl("Negative", colnames(counts)) & !grepl("SystemControl", colnames(counts))]


```



Lets take a look at the structure of the dataset we've assembled:
```{r lookatstructure}
atomxdata <- list(counts = counts,
                  metadata = metadata,
                  negcounts = negcounts,
                  falsecounts = falsecounts)
str(atomxdata)
```

Now our data looks like this:

![](img/data1.png)


Next we save the data as .RDS files. These files are smaller on disk (they 
benefit from more compression than flat files do), and they're faster to read
back into R.

**Please note that these files hold intermediate results for convenience during analysis; they are not official file formats supported by NanoString.**

```{r saving}
if(!dir.exists(paste0(outdir, "/processed_data"))){
  dir.create(paste0(outdir, "/processed_data"))
}
saveRDS(atomxdata$counts, file = paste0(outdir, "/processed_data/counts_unfiltered.RDS"))
saveRDS(atomxdata$negcounts, file = paste0(outdir, "/processed_data/negcounts_unfiltered.RDS"))
saveRDS(atomxdata$falsecounts, file = paste0(outdir, "/processed_data/falsecounts_unfiltered.RDS"))
saveRDS(atomxdata$metadata, file = paste0(outdir, "/processed_data/metadata_unfiltered.RDS"))
```

Now we have data in 3 locations:

- The RDS files we just created
- The flat files we began with
- The original data on AtoMx

This is unnecessary duplication, and since CosMx data is so large, storing extra copies can incur real cost. 
There are two feasible approaches:

1. Once you've finished your analysis, delete all the large .RDS files created during it. 
They can always be regenerated by re-running the analysis scripts. 
If you are computing on the cloud, re-running does incur some compute costs, but these are minor compared to the cost of 
indefinitely storing large files. 

2. Once you've confirmed this script data loading script has run successfully,
you can safely delete the flat files - you won't be using them again. 
If you do need to go back to them, you can always export them from AtoMx again:
AtoMx acts as a persistent, authoritative source of unmodified data.

### Note on data size

A reasonably high-performance compute instance can generally handle a few slides of 
 CosMx data using the scripts we use here. But once studies get to millions of cells, they
 can overwhelm the memory available to most analysts. At this point, you have two options:

1. Use data formats that store data on disk, not in memory, e.g. TileDB, SeuratDisk, hdf5... 
2. Analyze your data one tissue at a time. Put all tissues through a standard analysis pipeline, then, for multi-tissue analyses, judiciously load in just the data you need
for a given analysis. 

Neither option is terribly convenient, but having millions or tens of millions of cells 
to analyze is worth some hassle. 


